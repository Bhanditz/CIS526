#!/usr/bin/env python
import itertools
import math
import optparse
import operator
import random
import sys
import bleu
from nltk.util import ngrams
from collections import namedtuple
from nltk.tree import Tree, ParentedTree
from nltk.tag import pos_tag

# Simplified implementation of Pairwise Ranking Optimization (PRO) due to
# Hopkins & May (2011): http://aclweb.org/anthology-new/D/D11/D11-1125.pdf
#
# Differences from paper:
# -Does not include candidate generation (candidates are assumed to be given)
# -Uses perceptron instead of maxent for classification

translation_candidate = namedtuple("candidate", "features, smoothed_bleu")
optparser = optparse.OptionParser()
optparser.add_option("-r", "--reference", dest="reference", default="data/train.en", help="English reference sentences")
optparser.add_option("-n", "--nbest", dest="nbest", default="data/train.nbest.shuffled", help="N-best lists")
optparser.add_option("-s", "--nbest_test", dest="nbest_test", default="data/test.nbest.shuffled", help="N-best lists")
optparser.add_option("-m", "--num-training-sentences", dest="m", default=sys.maxint, type="int", help="Number of training sentences (default=all)")
optparser.add_option("-p", "--parses", dest="parses", default='data/new-oneline-parse.txt', help="parallel file containing parses")
optparser.add_option("-t", "--pos-tags", dest="pos", default='data/pos-tags.txt', help="parallel file containing parses")
(opts,_) = optparser.parse_args()

sys.stderr.write("Reading N-best lists...")

feature_num = 7
wordidx = {}

parses = [l.strip() for l in open(opts.parses).readlines()]
pos = [l.strip() for l in open(opts.pos).readlines()]

all_sents = [l for l in open(opts.nbest)] + [l for l in open(opts.nbest_test)]

for n, line in enumerate(all_sents) : #open(opts.nbest)):
  (i, sentence, features) = line.strip().split("|||")
  (i, features) = (int(i), {k+1:float(h) for k,h in enumerate(features.strip().split())})
  if i >= opts.m:
    break
  #unigram features
  for w in sentence.split() : 
	  if w not in wordidx : wordidx[w] = feature_num; feature_num += 1
	  widx = wordidx[w] 
	  if widx not in features : features[widx] = 0 
	  features[widx] += 1
  #unicode check
  try : sentence.encode('ascii') 
  except UnicodeDecodeError : 
	  if 'unicode-error' not in wordidx : wordidx['unicode-error'] = feature_num; feature_num += 1
	  widx = wordidx['unicode-error']
	  if widx not in features : features[widx] = 0 
	  features[widx] += 1
  for k in [1, 2, 3] : 
	  for w in ngrams(pos[n].split(),k) : 
		  w = w[0]
		  if w not in wordidx : wordidx[w] = feature_num; feature_num += 1
		  widx = wordidx[w] 
		  if widx not in features : features[widx] = 0 
		  features[widx] += 1
  for k in [1, 2, 3] : 
	  for w in ngrams(pos[n].split(),k) : 
		  if w not in wordidx : wordidx[w] = feature_num; feature_num += 1
		  widx = wordidx[w] 
		  if widx not in features : features[widx] = 0 
		  features[widx] += 1
  if not(parses[n] == 'None') : 
	ptree = Tree(parses[n])
	for w in ptree.productions() : 
		if w not in wordidx : wordidx[w] = feature_num; feature_num += 1
		widx = wordidx[w] 
		if widx not in features : features[widx] = 0 
		features[widx] += 1
  else : 
	w = 'no parse'
	if w not in wordidx : wordidx[w] = feature_num; feature_num += 1
	widx = wordidx[w] 
	if widx not in features : features[widx] = 0 
	features[widx] += 1
  print '0\t%s'%('\t'.join(['%d:%f'%(k,h) for k,h in sorted(features.iteritems(), key=operator.itemgetter(0))]))
  if n % 2000 == 0:
    sys.stderr.write(".")
sys.stderr.write("\n")


